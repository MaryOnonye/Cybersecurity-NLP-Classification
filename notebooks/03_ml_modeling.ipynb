{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Machine Learning Modeling – Cybersecurity News Classification\n",
    "\n",
    "## Objective\n",
    "Train and evaluate traditional machine learning models for classifying cybersecurity news articles into threat categories.\n",
    "\n",
    "## Models Evaluated\n",
    "- Logistic Regression\n",
    "- Linear SVM\n",
    "- Random Forest (if applicable)\n",
    "- TF-IDF (word + character n-grams)\n",
    "\n",
    "## Evaluation Metrics\n",
    "- Accuracy\n",
    "- Precision / Recall\n",
    "- F1 Score (macro + weighted)\n",
    "- Confusion Matrix\n",
    "\n",
    "## Goal\n",
    "Identify the strongest baseline model before comparing against deep learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist, ConfusionMatrix\n",
    "from nltk.classify import NaiveBayesClassifier, DecisionTreeClassifier, MaxentClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(\"../data/TheHackerNews_Dataset.xlsx\")\n",
    "df.head()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_COL = \"Title\"\n",
    "TEXT_COL  = \"Article\"\n",
    "LABEL_COL = \"Label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant columns\n",
    "df = df[[TITLE_COL, TEXT_COL, LABEL_COL]].copy()\n",
    "\n",
    "# Drop rows with missing data\n",
    "df.dropna(subset=[TEXT_COL, LABEL_COL], inplace=True)\n",
    "\n",
    "# Remove duplicate text\n",
    "df.drop_duplicates(subset=[TEXT_COL], inplace=True)\n",
    "\n",
    "print(\"Shape after cleaning:\", df.shape)\n",
    "label_counts = df[LABEL_COL].value_counts()\n",
    "rare_labels = label_counts[label_counts < 5].index\n",
    "df = df[~df[LABEL_COL].isin(rare_labels)]\n",
    "\n",
    "print(\"Shape after removing rare labels:\", df.shape)\n",
    "print(df[LABEL_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "url_pattern = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "non_alpha_pattern = re.compile(r\"[^a-zA-Z0-9]+\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = url_pattern.sub(\" \", text)\n",
    "    # Simple cleanup of weird characters, multiple spaces etc.\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation, stopwords and lemmatize\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        if tok in string.punctuation:\n",
    "            continue\n",
    "        if tok in stop_words:\n",
    "            continue\n",
    "        # strip non-alphanumeric stuff\n",
    "        tok = non_alpha_pattern.sub(\"\", tok)\n",
    "        if tok == \"\":\n",
    "            continue\n",
    "        lemma = lemmatizer.lemmatize(tok)\n",
    "        clean_tokens.append(lemma)\n",
    "    \n",
    "    return clean_tokens\n",
    "\n",
    "# Combine title + article into a raw text column (for IOC regexes)\n",
    "df[\"raw_text\"] = (df[TITLE_COL].fillna(\"\") + \" \" + df[TEXT_COL].fillna(\"\"))\n",
    "\n",
    "# Preprocess raw_text into tokens\n",
    "df[\"tokens\"] = df[\"raw_text\"].apply(preprocess_text)\n",
    "\n",
    "# Drop very short docs based on tokens\n",
    "df = df[df[\"tokens\"].apply(lambda toks: len(toks) >= 20)].copy()\n",
    "print(\"Shape after removing very short docs:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [tok for doc in df[\"tokens\"] for tok in doc]\n",
    "freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "# build a bigram vocabulary \n",
    "all_bigrams = ['_'.join(bg) for doc in df[\"tokens\"] for bg in bigrams(doc)]\n",
    "bigram_fd = FreqDist(all_bigrams)\n",
    "TOP_BIGRAMS = 300  # you can play with 200–500 depending on speed vs performance\n",
    "bigram_features = [bg for bg, _ in bigram_fd.most_common(TOP_BIGRAMS)]\n",
    "\n",
    "# Smaller vocab for NLTK classifiers\n",
    "N_FEATURES = 1500  # unigram count\n",
    "\n",
    "# Keep frequent tokens, but drop very generic ones\n",
    "generic = {\"security\", \"user\", \"system\", \"data\", \"also\", \"company\", \"information\"}\n",
    "unigram_features = [\n",
    "    w for (w, _) in freq_dist.most_common(N_FEATURES * 2)\n",
    "    if w not in generic\n",
    "][:N_FEATURES]\n",
    "\n",
    "# Combine unigrams + bigrams into a single feature list\n",
    "word_features = unigram_features + bigram_features\n",
    "\n",
    "len(word_features), word_features[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ransom_keywords  = {\"ransomware\", \"ransom\", \"decryptor\", \"decryption\", \"extortion\"}\n",
    "phish_keywords   = {\"phishing\", \"spearphishing\", \"spear-phishing\", \"credential\", \"credentials\"}\n",
    "vuln_keywords    = {\"cve\", \"zero-day\", \"zeroday\", \"0-day\", \"vulnerability\", \"exploit\"}\n",
    "malware_keywords = {\"malware\", \"trojan\", \"backdoor\", \"rootkit\", \"spyware\", \"infostealer\", \"botnet\"}\n",
    "cloud_keywords   = {\"cloud\", \"aws\", \"azure\", \"gcp\", \"s3\", \"bucket\", \"kubernetes\", \"container\"}\n",
    "attack_keywords = {\n",
    "    \"attack\", \"attacks\", \"cyberattack\", \"cyber-attack\",\n",
    "    \"ddos\", \"denial-of-service\", \"dos\",\n",
    "    \"campaign\", \"operation\", \"intrusion\", \"espionage\",\n",
    "    \"apt\", \"advanced persistent threat\"\n",
    "}\n",
    "\n",
    "breach_keywords = {\n",
    "    \"breach\", \"breached\", \"leak\", \"leaked\", \"leaking\",\n",
    "    \"exposed\", \"exposure\", \"compromised\", \"data breach\"\n",
    "}\n",
    "\n",
    "# IOC regexes\n",
    "ip_pattern    = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
    "cve_pattern   = re.compile(r\"\\bCVE-\\d{4}-\\d{4,7}\\b\", re.I)\n",
    "hash_pattern  = re.compile(r\"\\b[0-9a-f]{32,64}\\b\", re.I)\n",
    "email_pattern = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\")\n",
    "\n",
    "def extra_features(tokens, raw_text=None):\n",
    "    num_tokens = len(tokens)\n",
    "    avg_len = (sum(len(t) for t in tokens) / num_tokens) if num_tokens > 0 else 0\n",
    "\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    # Length buckets (categorical flags) \n",
    "    long_doc   = (num_tokens > 200)\n",
    "    medium_doc = (100 < num_tokens <= 200)\n",
    "    short_doc  = (num_tokens <= 100)\n",
    "    avg_long   = (avg_len > 6)\n",
    "    avg_medium = (4 < avg_len <= 6)\n",
    "    avg_short  = (avg_len <= 4)\n",
    "\n",
    "    # Domain keyword booleans \n",
    "    has_ransom  = (len(token_set & ransom_keywords) > 0)\n",
    "    has_phish   = (len(token_set & phish_keywords) > 0)\n",
    "    has_vuln    = (len(token_set & vuln_keywords) > 0)\n",
    "    has_malware = (len(token_set & malware_keywords) > 0)\n",
    "    has_cloud   = (len(token_set & cloud_keywords) > 0)\n",
    "    has_attack  = (len(token_set & attack_keywords) > 0)\n",
    "    has_breach  = (len(token_set & breach_keywords) > 0)\n",
    "\n",
    "    # IOC regex booleans (use raw_text if available) \n",
    "    if raw_text is None:\n",
    "        raw_text = \" \".join(tokens)\n",
    "\n",
    "    has_ip    = bool(ip_pattern.search(raw_text))\n",
    "    has_cve   = bool(cve_pattern.search(raw_text))\n",
    "    has_hash  = bool(hash_pattern.search(raw_text))\n",
    "    has_email = bool(email_pattern.search(raw_text))\n",
    "\n",
    "    # POS-based proportion buckets \n",
    "    tagged = nltk.pos_tag(tokens) if num_tokens > 0 else []\n",
    "    total = len(tagged) or 1\n",
    "    num_nnp = sum(1 for _, t in tagged if t == \"NNP\")\n",
    "    num_nn  = sum(1 for _, t in tagged if t.startswith(\"NN\"))\n",
    "    num_vb  = sum(1 for _, t in tagged if t.startswith(\"VB\"))\n",
    "    prop_nnp = num_nnp / total\n",
    "    prop_nn  = num_nn / total\n",
    "    prop_vb  = num_vb / total\n",
    "\n",
    "    many_proper_nouns = (prop_nnp > 0.2)\n",
    "    verb_heavy        = (prop_vb  > 0.2)\n",
    "    noun_heavy        = (prop_nn  > 0.3)\n",
    "\n",
    "    return {\n",
    "        # length buckets\n",
    "        \"long_doc\": long_doc,\n",
    "        \"medium_doc\": medium_doc,\n",
    "        \"short_doc\": short_doc,\n",
    "        \"avg_long_word\": avg_long,\n",
    "        \"avg_medium_word\": avg_medium,\n",
    "        \"avg_short_word\": avg_short,\n",
    "        # domain keywords\n",
    "        \"has_ransom\": has_ransom,\n",
    "        \"has_phish\": has_phish,\n",
    "        \"has_vuln\": has_vuln,\n",
    "        \"has_malware\": has_malware,\n",
    "        \"has_cloud\": has_cloud,\n",
    "        \"has_attack\": has_attack,\n",
    "        \"has_breach\": has_breach,\n",
    "        # IOC-style pattern flags\n",
    "        \"has_ip\": has_ip,\n",
    "        \"has_cve\": has_cve,\n",
    "        \"has_hash\": has_hash,\n",
    "        \"has_email\": has_email,\n",
    "        # POS buckets\n",
    "        \"many_proper_nouns\": many_proper_nouns,\n",
    "        \"verb_heavy\": verb_heavy,\n",
    "        \"noun_heavy\": noun_heavy,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(tokens, raw_text=None):\n",
    "    token_set = set(tokens)\n",
    "    bigram_set = set('_'.join(bg) for bg in bigrams(tokens))\n",
    "    features = {}\n",
    "\n",
    "    # Bag-of-words presence (binary); support uni + bi\n",
    "    for w in word_features:\n",
    "        if \"_\" in w:\n",
    "            # treat as bigram token, e.g. \"zero_day\"\n",
    "            features[f\"bi={w}\"] = (w in bigram_set)\n",
    "        else:\n",
    "            # treat as unigram token\n",
    "            features[f\"uni={w}\"] = (w in token_set)\n",
    "\n",
    "    # Extra domain + length + IOC + POS flags\n",
    "    features.update(extra_features(tokens, raw_text))\n",
    "\n",
    "    return features\n",
    "\n",
    "# Inspect one example\n",
    "example_feats = document_features(df[\"tokens\"].iloc[0])\n",
    "list(example_feats.items())[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [\n",
    "    (document_features(tokens, raw_text), label)\n",
    "    for tokens, raw_text, label in zip(df[\"tokens\"], df[\"raw_text\"], df[LABEL_COL])\n",
    "]\n",
    "\n",
    "len(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(feature_sets)\n",
    "\n",
    "n_total = len(feature_sets)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_dev   = int(0.1 * n_total)\n",
    "\n",
    "train_set = feature_sets[:n_train]\n",
    "dev_set   = feature_sets[n_train:n_train + n_dev]\n",
    "test_set  = feature_sets[n_train + n_dev:]\n",
    "\n",
    "len(train_set), len(dev_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffled the full dataset and then split it into 80% training, 10% development, \n",
    "and 10% test because each portion plays a different role in building a reliable classifier. \n",
    "The training set is used to learn patterns from the text, the development set is used to tune \n",
    "the model and compare different classifiers, and the test set is kept completely separate \n",
    "until the end so we can measure performance on truly unseen data.\n",
    "This split helps prevent overfitting, makes our model comparisons fair, \n",
    "and gives an unbiased estimate of how well the classifier is likely to perform in real-world use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Naive Bayes...\")\n",
    "nb_classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(\"Training Decision Tree...\")\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set)\n",
    "\n",
    "print(\"Training MaxEnt...\")\n",
    "me_classifier = MaxentClassifier.train(\n",
    "    train_set,\n",
    "    algorithm='GIS',   # GIS more stable than IIS on sparse data\n",
    "    max_iter=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train all three models so we can compare:\n",
    "which model has the best accuracy\n",
    "which model handles the text features best\n",
    "how different classifiers behave on the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Dev set accuracy ===\")\n",
    "print(\"Naive Bayes dev accuracy:\", accuracy(nb_classifier, dev_set))\n",
    "print(\"Decision Tree dev accuracy:\", accuracy(dt_classifier, dev_set))\n",
    "print(\"MaxEnt dev accuracy:\", accuracy(me_classifier, dev_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluated all three classifiers on the development set so we could compare their performance before \n",
    "using the test set. The development set helps identify which model generalizes better without touching \n",
    "the final test data. By checking dev accuracy for Naive Bayes, Decision Tree, and MaxEnt, we can see \n",
    "which classifier is learning the patterns most effectively. In this case, the MaxEnt classifier \n",
    "performed the best on the dev set, so it is the strongest candidate to evaluate on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a test set even after using a dev set because the dev set is used during model selection and tuning.\n",
    "Once we choose the best model using the development set, we evaluate it on the test set to get an unbiased \n",
    "estimate of how well it performs on completely unseen data. The test set represents the final check to \n",
    "make sure the model generalizes and is not overfitting to the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Test set accuracy ===\")\n",
    "print(\"Naive Bayes test accuracy:\", accuracy(nb_classifier, test_set))\n",
    "print(\"Decision Tree test accuracy:\", accuracy(dt_classifier, test_set))\n",
    "print(\"MaxEnt test accuracy:\", accuracy(me_classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the best model using the development set, we evaluated all three classifiers on the test set\n",
    "to measure their final performance on unseen data. The test set is completely separate from training and \n",
    "tuning, so it provides an unbiased estimate of real-world accuracy. \n",
    "The results show that the MaxEnt classifier achieves the highest test accuracy, followed by the Decision \n",
    "Tree and Naive Bayes models. This confirms that MaxEnt generalizes the best to new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart theme\n",
    "plt.rcParams.update({\n",
    "\n",
    "    # Backgrounds\n",
    "    \"figure.facecolor\": \"#0B0F10\",\n",
    "    \"axes.facecolor\":   \"#0B0F10\",\n",
    "\n",
    "    # Text & labels\n",
    "    \"text.color\":       \"#D0D6D8\",\n",
    "    \"axes.labelcolor\":  \"#D0D6D8\",\n",
    "    \"xtick.color\":      \"#A8B2B5\",\n",
    "    \"ytick.color\":      \"#A8B2B5\",\n",
    "\n",
    "    # Lines & markers (neon theme)\n",
    "    \"axes.prop_cycle\": plt.cycler(color=[\"#4EFF8E\", \"#4EC9FF\", \"#FFDD4E\", \"#FF4E4E\"]),\n",
    "\n",
    "    # Grid\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.color\": \"#2A3236\",\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 0.4,\n",
    "    \"grid.alpha\": 0.6,\n",
    "\n",
    "    # Spines\n",
    "    \"axes.edgecolor\": \"#3A4144\",\n",
    "    \"axes.linewidth\": 1.2,\n",
    "\n",
    "    # Titles & fonts\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"font.size\": 12,\n",
    "    \"font.family\": \"DejaVu Sans\",\n",
    "\n",
    "    # Legend\n",
    "    \"legend.facecolor\": \"#0E1315\",\n",
    "    \"legend.edgecolor\": \"#2A3236\",\n",
    "    \"legend.framealpha\": 0.8,\n",
    "    \"legend.fontsize\": 10,\n",
    "\n",
    "    # Saving figures\n",
    "    \"savefig.facecolor\": \"#0B0F10\",\n",
    "    \"savefig.edgecolor\": \"#0B0F10\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = me_classifier\n",
    "\n",
    "gold = [label for (feats, label) in test_set]\n",
    "pred = [best_classifier.classify(feats) for (feats, label) in test_set]\n",
    "\n",
    "cm = ConfusionMatrix(gold, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nltk_classifier(clf, dataset, name=\"Classifier\"):\n",
    "    y_true = [y for (_, y) in dataset]\n",
    "    y_pred = [clf.classify(x) for (x, _) in dataset]\n",
    "    print(f\"\\n=== {name} ({clf.__class__.__name__}) ===\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Macro-F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    print(\"Weighted-F1:\", f1_score(y_true, y_pred, average=\"weighted\"))\n",
    "\n",
    "# DEV vs TEST F1 for all three\n",
    "eval_nltk_classifier(nb_classifier, dev_set, \"NaiveBayes DEV\")\n",
    "eval_nltk_classifier(nb_classifier, test_set, \"NaiveBayes TEST\")\n",
    "eval_nltk_classifier(dt_classifier, dev_set, \"DecisionTree DEV\")\n",
    "eval_nltk_classifier(dt_classifier, test_set, \"DecisionTree TEST\")\n",
    "eval_nltk_classifier(me_classifier, dev_set, \"MaxEnt DEV\")\n",
    "eval_nltk_classifier(me_classifier, test_set, \"MaxEnt TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"NB\", \"DT\", \"MaxEnt\", \"LogReg\",\n",
    "    \"RF\", \"GB\", \"KNN\", \"LogReg_Tuned\"\n",
    "]\n",
    "\n",
    "accuracies = [\n",
    "    0.70, 0.71, 0.73,\n",
    "    0.812, 0.814, 0.8306,\n",
    "    0.756, 0.833\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(model_names, accuracies, color=\"#4EC9FF\")\n",
    "plt.title(\"Model Comparison – Test Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.5, 0.9)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar chart compares the accuracy of all eight models we tested. It shows that the classical sklearn \n",
    "models perform better than the NLTK baselines, with tuned Logistic Regression achieving the highest accuracy \n",
    "overall. This visual helps highlight the performance gap between simpler models and more optimized approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_f1 = [\n",
    "    0.67, 0.70, 0.73,\n",
    "    0.76, 0.68, 0.77, 0.66, 0.78\n",
    "]\n",
    "\n",
    "weighted_f1 = [\n",
    "    0.75, 0.76, 0.78,\n",
    "    0.81, 0.78, 0.82, 0.75, 0.83\n",
    "]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(x - width/2, macro_f1, width, label=\"Macro F1\")\n",
    "plt.bar(x + width/2, weighted_f1, width, label=\"Weighted F1\")\n",
    "plt.xticks(x, model_names)\n",
    "plt.title(\"Macro F1 vs Weighted F1 Across Models\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a confusion matrix to see exactly where the classifier is making correct predictions and where \n",
    "it is making mistakes. Accuracy alone does not show which classes are performing well or poorly, \n",
    "so the confusion matrix helps reveal detailed patterns. By comparing the true labels with the predicted \n",
    "labels, we can see which categories the model confuses, such as Cyber_Attack being misclassified as \n",
    "Malware or Vulnerability. This analysis helps explain the model’s weaknesses and guides improvements \n",
    "for future versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MaxEnt classifier performs best on the Malware and Vulnerability classes, achieving many correct \n",
    "predictions. However, it struggles with the Cyber_Attack category, often confusing it with Malware or \n",
    "Vulnerability. This likely happens because the vocabulary in cybersecurity news overlaps heavily across \n",
    "categories, making Cyber_Attack harder to distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examined the most informative features of the MaxEnt classifier to understand which words were driving \n",
    "its predictions. This helps explain why the model chooses certain labels and reveals whether the classifier \n",
    "learned meaningful cybersecurity terms. Many high-weight words, such as “xss,” “router,” “overflow,” and \n",
    "“zeus,” align well with real-world cybersecurity concepts. This confirms that the model is learning \n",
    "useful patterns rather than random noise, and it also helps explain some of the errors by showing \n",
    "which features influence each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Features for Sklearn Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to use integer indexing for domain features\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Raw text for TF-IDF: join tokens back into a string\n",
    "df[\"text_for_tfidf\"] = df[\"tokens\"].apply(lambda toks: \" \".join(toks))\n",
    "y = df[LABEL_COL].values\n",
    "X_text = df[\"text_for_tfidf\"].values\n",
    "\n",
    "# Train/dev/test split: 70% train, 15% dev, 15% test\n",
    "X_train_text, X_temp_text, y_train, y_temp, train_idx, temp_idx = train_test_split(\n",
    "    X_text, y, np.arange(len(y)),\n",
    "    test_size=0.30, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_dev_text, X_test_text, y_dev, y_test, dev_idx, test_idx = train_test_split(\n",
    "    X_temp_text, y_temp, temp_idx,\n",
    "    test_size=0.50, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "X_train_text.shape, X_dev_text.shape, X_test_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we reset the DataFrame index and converted the token lists back into text strings so that we could use \n",
    "TfidfVectorizer on the cleaned article text. Then we split the data into 70% training, 15% development, and \n",
    "15% test sets using stratified sampling. The training set is used to fit the models, the development set \n",
    "is used to tune and compare models, and the test set is held out until the end to measure final performance \n",
    "on unseen data. This setup makes sure the models are trained fairly and evaluated in a way that reflects \n",
    "real-world generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),    # unigrams + bigrams + trigrams\n",
    "    min_df=3,\n",
    "    max_features=30000,\n",
    "    sublinear_tf=True,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_dev_tfidf   = tfidf.transform(X_dev_text)\n",
    "X_test_tfidf  = tfidf.transform(X_test_text)\n",
    "\n",
    "X_train_tfidf.shape, X_dev_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tfidf = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 6),\n",
    "    min_df=3\n",
    ")\n",
    "\n",
    "X_train_char = char_tfidf.fit_transform(X_train_text)\n",
    "X_dev_char   = char_tfidf.transform(X_dev_text)\n",
    "X_test_char  = char_tfidf.transform(X_test_text)\n",
    "\n",
    "# Concatenate word- and char-level TF-IDF\n",
    "X_train_tfidf_all = hstack([X_train_tfidf, X_train_char])\n",
    "X_dev_tfidf_all   = hstack([X_dev_tfidf,   X_dev_char])\n",
    "X_test_tfidf_all  = hstack([X_test_tfidf,  X_test_char])\n",
    "\n",
    "X_train_tfidf_all.shape, X_dev_tfidf_all.shape, X_test_tfidf_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dicts = [\n",
    "    extra_features(tokens, raw_text)\n",
    "    for tokens, raw_text in zip(df[\"tokens\"], df[\"raw_text\"])\n",
    "]\n",
    "dv = DictVectorizer(sparse=True)\n",
    "X_all_domain = dv.fit_transform(domain_dicts)\n",
    "\n",
    "\n",
    "X_train_dom = X_all_domain[train_idx]\n",
    "X_dev_dom   = X_all_domain[dev_idx]\n",
    "X_test_dom  = X_all_domain[test_idx]\n",
    "\n",
    "X_train_dom.shape, X_dev_dom.shape, X_test_dom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([X_train_tfidf_all, X_train_dom])\n",
    "X_dev   = hstack([X_dev_tfidf_all,   X_dev_dom])\n",
    "X_test  = hstack([X_test_tfidf_all,  X_test_dom])\n",
    "\n",
    "X_train.shape, X_dev.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        n_jobs=1,\n",
    "        class_weight='balanced',\n",
    "        solver=\"lbfgs\"\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        n_jobs=1,\n",
    "        class_weight='balanced_subsample',\n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a dictionary of four different sklearn classifiers so we could train and evaluate multiple \n",
    "supervised learning models using the same TF-IDF features. Each model represents a different type of learning\n",
    "approach, such as linear models, ensemble methods, boosting, and instance-based learning. By organizing \n",
    "them in a dictionary, we can easily loop through each model, fit it on the training data, and compare \n",
    "their performance on the development and test sets. This makes it straightforward to identify which model \n",
    "performs best for our cybersecurity text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_results = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n====================\")\n",
    "    print(f\"Training {name} ...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Dev set evaluation\n",
    "    y_dev_pred = clf.predict(X_dev)\n",
    "    dev_acc = accuracy_score(y_dev, y_dev_pred)\n",
    "    dev_f1_macro = f1_score(y_dev, y_dev_pred, average=\"macro\")\n",
    "    dev_f1_weighted = f1_score(y_dev, y_dev_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"{name} DEV accuracy: {dev_acc:.4f}\")\n",
    "    print(f\"{name} DEV macro-F1: {dev_f1_macro:.4f}\")\n",
    "    print(f\"{name} DEV weighted-F1: {dev_f1_weighted:.4f}\")\n",
    "    print(\"\\nDEV Classification Report:\")\n",
    "    print(classification_report(y_dev, y_dev_pred))\n",
    "\n",
    "    # Test set evaluation\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1_macro = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "    test_f1_weighted = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"{name} TEST accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{name} TEST macro-F1: {test_f1_macro:.4f}\")\n",
    "    print(f\"{name} TEST weighted-F1: {test_f1_weighted:.4f}\")\n",
    "    print(\"\\nTEST Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    sk_results[name] = {\n",
    "        \"model\": clf,\n",
    "        \"dev_acc\": dev_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"dev_f1_macro\": dev_f1_macro,\n",
    "        \"test_f1_macro\": test_f1_macro,\n",
    "        \"dev_f1_weighted\": dev_f1_weighted,\n",
    "        \"test_f1_weighted\": test_f1_weighted,\n",
    "        \"dev_pred\": y_dev_pred,\n",
    "        \"test_pred\": y_test_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We evaluated each sklearn model on both the development set and the test set so we could compare their \n",
    "performance in a fair and consistent way. The development set allowed me to see how well each classifier \n",
    "generalizes during model selection without touching the final test data. After comparing dev accuracy and \n",
    "F1 scores, we then evaluated the models on the test set to obtain an unbiased measurement of real-world \n",
    "performance. This process showed that Gradient Boosting performed the best among the untuned models, \n",
    "while the tuned Logistic Regression achieved the highest accuracy overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_f1 = [\"Logistic Regression\",\"Random Forest\",\n",
    "                  \"Gradient Boosting\",\"KNN\",\"Tuned Logistic Regression\"]\n",
    "\n",
    "macro_f1 = [0.76,0.68,0.77,0.66,0.78]\n",
    "weighted_f1 = [0.81,0.78,0.82,0.75,0.83]\n",
    "\n",
    "x = np.arange(len(model_names_f1))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(x - width/2, macro_f1, width, label=\"Macro F1\")\n",
    "plt.bar(x + width/2, weighted_f1, width, label=\"Weighted F1\")\n",
    "plt.xticks(x, model_names_f1, rotation=45)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Macro F1 vs Weighted F1 (Test Set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart compares Macro and Weighted F1 scores for all models. Tuned Logistic Regression clearly performs \n",
    "the best overall, followed by Gradient Boosting. Random Forest and KNN have lower Macro F1, which shows \n",
    "they struggle with the minority Cyber_Attack class. The weighted F1 scores are higher because the dataset is \n",
    "imbalanced, with Malware and Vulnerability dominating. Overall, Logistic Regression gives us the strongest \n",
    "and most balanced performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"Gradient Boosting\", \"KNN\"],\n",
    "    \"Dev Accuracy\": [0.7955, 0.8161, 0.8182, 0.7500],\n",
    "    \"Test Accuracy\": [0.8120, 0.8140, 0.8306, 0.7562]\n",
    "}\n",
    "\n",
    "df_compare = pd.DataFrame(data)\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameter tuning\n",
    "# Combine train + dev for tuning\n",
    "X_tune = vstack([X_train, X_dev])\n",
    "y_tune = np.concatenate([np.array(y_train), np.array(y_dev)])\n",
    "\n",
    "X_tune.shape, len(y_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#Logistic Regression Grid Search\n",
    "log_reg_base = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    solver=\"lbfgs\"\n",
    ")\n",
    "\n",
    "log_reg_param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "log_reg_grid = GridSearchCV(\n",
    "    estimator=log_reg_base,\n",
    "    param_grid=log_reg_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"f1_macro\", \n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Running GridSearchCV for Logistic Regression...\")\n",
    "log_reg_grid.fit(X_tune, y_tune)\n",
    "\n",
    "print(\"\\nBest LogisticRegression params:\", log_reg_grid.best_params_)\n",
    "print(\"Best CV score:\", log_reg_grid.best_score_)\n",
    "\n",
    "best_log_reg = log_reg_grid.best_estimator_\n",
    "\n",
    "y_test_pred_lr = best_log_reg.predict(X_test)\n",
    "print(\"\\n=== Tuned Logistic Regression on TEST ===\")\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_test_pred_lr))\n",
    "print(classification_report(y_test, y_test_pred_lr))\n",
    "test_f1_macro_lr = f1_score(y_test, y_test_pred_lr, average=\"macro\")\n",
    "test_f1_weighted_lr = f1_score(y_test, y_test_pred_lr, average=\"weighted\")\n",
    "print(\"Test macro-F1:\", test_f1_macro_lr)\n",
    "print(\"Test weighted-F1:\", test_f1_weighted_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Model\": [\n",
    "        \"Logistic Regression (default)\",\n",
    "        \"Logistic Regression (tuned)\"\n",
    "    ],\n",
    "    \"Test Accuracy\": [0.8120, 0.8326],\n",
    "    \"Macro F1\": [0.76, 0.78],\n",
    "    \"Weighted F1\": [0.81, 0.83]\n",
    "}\n",
    "\n",
    "df_lr_compare = pd.DataFrame(data)\n",
    "df_lr_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tuned Logistic Regression using GridSearchCV to find the best hyperparameters. Grid search identified \n",
    "C=10 and class_weight='balanced' as the optimal combination. After tuning, Logistic Regression achieved \n",
    "the highest classical model accuracy at 83.26% and strong F1 scores across all classes. This makes it the \n",
    "best baseline model before comparing with deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== NLTK Models (DEV / TEST) ===\")\n",
    "print(\"NaiveBayes (NLTK)  DEV:\", accuracy(nb_classifier, dev_set),\n",
    "      \" TEST:\", accuracy(nb_classifier, test_set))\n",
    "print(\"DecisionTree (NLTK) DEV:\", accuracy(dt_classifier, dev_set),\n",
    "      \" TEST:\", accuracy(dt_classifier, test_set))\n",
    "print(\"MaxEnt (NLTK)      DEV:\", accuracy(me_classifier, dev_set),\n",
    "      \" TEST:\", accuracy(me_classifier, test_set))\n",
    "\n",
    "print(\"\\n=== Sklearn Models (DEV / TEST) ===\")\n",
    "for name, res in sk_results.items():\n",
    "    print(f\"{name:20s} DEV: {res['dev_acc']:.4f} TEST: {res['test_acc']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Tuned Models (TEST) ===\")\n",
    "print(\"LogisticRegression (tuned) TEST acc:\",\n",
    "      accuracy_score(y_test, y_test_pred_lr))\n",
    "print(\"LogisticRegression (tuned) TEST macro-F1:\",\n",
    "      f1_score(y_test, y_test_pred_lr, average=\"macro\"))\n",
    "print(\"LogisticRegression (tuned) TEST weighted-F1:\",\n",
    "      f1_score(y_test, y_test_pred_lr, average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "rows = [\n",
    "    [\"**NLTK Models**\", \"\", \"\"],\n",
    "    [\"Naive Bayes (NLTK)\", 0.7764, 0.7028],\n",
    "    [\"Decision Tree (NLTK)\", 0.7857, 0.7059],\n",
    "    [\"MaxEnt (NLTK)\", 0.7981, 0.7307],\n",
    "    [\"\", \"\", \"\"],\n",
    "    [\"**Sklearn Models**\", \"\", \"\"],\n",
    "    [\"Logistic Regression\", 0.7955, 0.8120],\n",
    "    [\"Random Forest\", 0.8161, 0.8140],\n",
    "    [\"Gradient Boosting\", 0.8182, 0.8306],\n",
    "    [\"KNN\", 0.7500, 0.7562],\n",
    "    [\"\", \"\", \"\"],\n",
    "    [\"**Tuned Model**\", \"\", \"\"],\n",
    "    [\"Logistic Regression (tuned)\", None, 0.8326],\n",
    "]\n",
    "\n",
    "print(tabulate(rows, headers=[\"Model\", \"Dev Accuracy\", \"Test Accuracy\"], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"NB (NLTK)\", \"DT (NLTK)\", \"MaxEnt (NLTK)\",\n",
    "    \"LogReg\", \"RandomForest\", \"GradientBoost\", \"KNN\",\n",
    "    \"LogReg (tuned)\"\n",
    "]\n",
    "\n",
    "test_accuracies = [\n",
    "    0.7028, 0.7059, 0.7307,\n",
    "    0.8120, 0.8140, 0.8306, 0.7562,\n",
    "    0.8326\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    \"#b0b0b0\", \"#b0b0b0\", \"#b0b0b0\",   # NLTK models (light gray)\n",
    "    \"steelblue\",\"steelblue\",\"steelblue\",\"steelblue\",  # Sklearn models\n",
    "    \"mediumseagreen\"  # Tuned Logistic Regression highlighted  \n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(model_names, test_accuracies, color=colors)\n",
    "\n",
    "plt.title(\"Test Accuracy Comparison Across Models\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.ylim(0.65, 0.90)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuned Logistic Regression is the strongest and most balanced model, performing best on both frequent and \n",
    "rare classes, making it the top supervised model for our cybersecurity classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_results = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    ...\n",
    "    sk_results[name] = {\n",
    "        \"model\": clf,\n",
    "        \"dev_acc\": dev_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"dev_pred\": y_dev_pred,\n",
    "        \"test_pred\": y_test_pred\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 5, 10],\n",
    "    \"class_weight\": [\"balanced\", None]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, solver=\"lbfgs\"),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\"Running GridSearchCV for Logistic Regression...\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "tuned_lr = grid.best_estimator_\n",
    "\n",
    "print(\"Best LogisticRegression params:\", grid.best_params_)\n",
    "print(\"Best CV score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = sk_results[\"GradientBoosting\"][\"model\"]\n",
    "\n",
    "X_test_vec = X_test  \n",
    "\n",
    "classes = np.array([\"Cyber_Attack\", \"Data_Breaches\", \"Malware\", \"Vulnerability\"])\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "models = {\n",
    "    \"GradientBoosting\": gb_clf,\n",
    "    \"LogReg (tuned)\": tuned_lr,\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for idx, class_name in enumerate(classes):\n",
    "    plt.subplot(2, 2, idx + 1)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        y_score = model.predict_proba(X_test_vec)[:, idx]\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_test_bin[:, idx], y_score)\n",
    "        ap = average_precision_score(y_test_bin[:, idx], y_score)\n",
    "\n",
    "        plt.plot(recall, precision, label=f\"{model_name} (AP={ap:.2f})\")\n",
    "\n",
    "    plt.title(f\"Precision–Recall Curve: {class_name}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_table = pd.DataFrame({\n",
    "    \"Class\": [\"Cyber_Attack\", \"Data_Breaches\", \"Malware\", \"Vulnerability\"],\n",
    "    \"Gradient Boosting AP\": [0.58, 0.86, 0.91, 0.89],\n",
    "    \"Tuned Logistic Regression AP\": [0.55, 0.89, 0.90, 0.90]\n",
    "})\n",
    "\n",
    "ap_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "labels = np.array([\"Cyber_Attack\", \"Data_Breaches\", \"Malware\", \"Vulnerability\"])\n",
    "\n",
    "y_pred_lr = tuned_lr.predict(X_test)\n",
    "y_pred_gb = gb_clf.predict(X_test)\n",
    "\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr, labels=labels)\n",
    "cm_gb = confusion_matrix(y_test, y_pred_gb, labels=labels)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=labels)\n",
    "disp_lr.plot(ax=axes[0], cmap=\"Blues\", values_format='d')\n",
    "axes[0].set_title(\"Confusion Matrix – Tuned Logistic Regression\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "disp_gb = ConfusionMatrixDisplay(confusion_matrix=cm_gb, display_labels=labels)\n",
    "disp_gb.plot(ax=axes[1], cmap=\"Blues\", values_format='d')\n",
    "axes[1].set_title(\"Confusion Matrix – Gradient Boosting\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models perform well overall, but the tuned Logistic Regression shows a cleaner diagonal with fewer cross-category mistakes, \n",
    "especially for Malware and Vulnerability. Gradient Boosting does slightly better on Cyber Attack, but it produces more scattered errors, \n",
    "making Logistic Regression the more consistent and reliable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([\"Cyber_Attack\", \"Data_Breaches\", \"Malware\", \"Vulnerability\"])\n",
    "\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "y_score_lr = tuned_lr.predict_proba(X_test)\n",
    "y_score_gb = gb_clf.predict_proba(X_test)\n",
    "\n",
    "auc_lr = roc_auc_score(y_test_bin, y_score_lr, average=\"macro\", multi_class=\"ovr\")\n",
    "auc_gb = roc_auc_score(y_test_bin, y_score_gb, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "print(\"Logistic Regression macro AUC:\", auc_lr)\n",
    "print(\"Gradient Boosting macro AUC:\", auc_gb)\n",
    "\n",
    "models = [\"Logistic Regression (tuned)\", \"Gradient Boosting\"]\n",
    "auc_scores = [auc_lr, auc_gb]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "bars = plt.bar(models, auc_scores)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "             f\"{height:.3f}\", ha='center', fontsize=12)\n",
    "\n",
    "plt.ylabel(\"Macro ROC-AUC\")\n",
    "plt.title(\"Macro AUC Comparison – Logistic Regression vs Gradient Boosting\")\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Basic NLTK Pipeline -\n",
    "Tokenized text, removed stopwords, punctuation, numbers Extracted top unigrams → binary presence features Trained NLTK Naive Bayes, Decision Tree, MaxEnt\n",
    "\n",
    "These served as initial baselines but performed modestly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Domain Knowledge Features\n",
    "Added cybersecurity-specific boolean features from tokens: has_ransom, has_phish, has_vuln, has_malware, has_cloud\n",
    "\n",
    "Added document structure features: document length buckets (short_doc, medium_doc, long_doc) average word length buckets\n",
    "\n",
    "All models (NLTK and sklearn) improved significantly because of stronger signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Advanced Feature Engineering\n",
    "Added linguistic and security signals: IOC regex detection: has_ip, has_cve, has_hash, has_email POS style features: many_proper_nouns, verb_heavy, noun_heavy\n",
    "\n",
    "These features helped separate technical malware writeups from general news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Added Bigrams to Word Features\n",
    "Computed bigrams from tokens Added top bigrams to word_features Updated document_features() to support \"bi=\" Bigrams captured key cybersecurity phrases: “zero_day”, “supply_chain”, “remote_code_execution”\n",
    "\n",
    "This improved performance across both NLTK and sklearn models, especially RF and GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Switched Sklearn Models to TF-IDF\n",
    "Built TfidfVectorizer(ngram_range=(1,2)) Combined TF-IDF matrix with domain/IOC/POS features via hstack() Trained Logistic Regression, Random Forest, Gradient Boosting, KNN This produced the best performing models: GradientBoosting ~0.83+ LogisticRegression ~0.79–0.82\n",
    "\n",
    "Sklearn models benefited heavily from TF-IDF since they support continuous numeric vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Hyperparameter Tuning\n",
    "Ran GridSearchCV on Logistic Regression: Tuned C Tuned class_weight Combined train+dev for cross-validation\n",
    "\n",
    "Result: Final tuned Logistic Regression delivered strong improvements and validated the TF-IDF pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - We built a complete cybersecurity text-classification pipeline starting from NLTK binary features and progressively improved accuracy using domain knowledge, linguistic cues, bigrams, and TF-IDF weighting. We added IOC indicators, POS ratios, and document-structure features to strengthen signals. Transitioning sklearn models to TF-IDF plus these engineered features produced the best results, with GradientBoosting and tuned Logistic Regression achieving the highest accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
