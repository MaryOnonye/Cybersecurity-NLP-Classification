{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb75a41f",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling â€“ Cybersecurity News Articles\n",
    "\n",
    "## Objective\n",
    "Discover underlying themes in cybersecurity news articles using Latent Dirichlet Allocation (LDA) and interpret how topics align with threat categories.\n",
    "\n",
    "## What this notebook contains\n",
    "- Text preprocessing (cleaning, tokenization, lemmatization)\n",
    "- Bigram/trigram phrase detection\n",
    "- Dictionary + corpus construction\n",
    "- LDA model training and topic interpretation\n",
    "\n",
    "## Output\n",
    "Interpretable topic clusters that help explain label overlap and inform downstream classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup note:\n",
    "# If running locally in a fresh environment, install dependencies using:\n",
    "#   pip install -r requirements.txt\n",
    "# and download the spaCy model:\n",
    "#   python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931c0bd-224a-4155-ae06-b6b6662ee4a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STEP 2 - LOAD THE DATASET\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_excel(\"../data/TheHackerNews_Dataset.xlsx\")\n",
    "texts = df[\"Article\"].fillna(\"\").astype(str).tolist()\n",
    "print(texts[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aae16d-83b7-400a-b4b5-1ce3a35f6bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 - PREPOCESS USING SPACY ( TOKENIZE + REMOVE STOPWORDS + LEMMATIZE)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
    "\n",
    "def preprocess_many(text_list):\n",
    "    processed = []\n",
    "    \n",
    "    # nlp.pipe takes a list (or generator) of our texts and processes them in batches.\n",
    "    for doc in nlp.pipe((t.lower() for t in text_list), batch_size=20):\n",
    "        tokens = [\n",
    "            token.lemma_\n",
    "            for token in doc\n",
    "            if not token.is_stop\n",
    "            and not token.is_punct\n",
    "            and token.is_alpha\n",
    "            and len(token) > 3\n",
    "        ]\n",
    "        processed.append(tokens)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# Now we apply it to all our articles\n",
    "processed_docs = preprocess_many(texts)\n",
    "\n",
    "print(\"Number of documents:\", len(processed_docs))\n",
    "print(\"First doc tokens:\", processed_docs[0][:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a993116-9dd7-4aca-b3dd-cd9dd30e4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4  - CREATE DICTIONARY & CORPUS\n",
    "from gensim import corpora\n",
    "\n",
    "# Create dictionary from processed docs\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# remove very rare and very common words\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Create bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print(\"Vocabulary size:\", len(dictionary))\n",
    "print(\"Sample for first doc:\", corpus[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b03fac-c703-4515-9bae-dd0ba3f10039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 - RUN GENSIM LDA MODEL\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "num_topics = 6  \n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    alpha=\"auto\",\n",
    "    eta=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7be3e-7308-4d55-8d6a-9f2d4a4d64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6 - PRINT THE TOPICS\n",
    "\n",
    "for i, topic in lda_model.show_topics(num_topics=6, num_words=10, formatted=False):\n",
    "    print(f\"\\nTopic {i}:\")\n",
    "    print(\", \".join([word for word, prob in topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d412d6f-94ab-42c4-908b-41489c4ddda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Latent Dirichlet Allocation (LDA), six dominant topics emerged from our dataset:\n",
    "#(1) software vulnerabilities and patch releases,\n",
    "#(2) server-level threats and malicious campaigns,\n",
    "#(3) Android/mobile application security,\n",
    "#(4) ransomware and large-scale cyberattacks,\n",
    "#(5) corporate data breaches affecting customer information, and\n",
    "#(6) website/social media account compromises.\n",
    "# These themes align closely with real-world cybersecurity incidents commonly reported in technology news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd0da7-a54b-4014-a03f-7c487fc6d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7 - COMPARE LABELS & GET DOMINANT TOPIC PER DOCUMENT\n",
    "\n",
    "def get_dominant_topic(model, bow_doc):\n",
    "    \"\"\"\n",
    "    For one document in bag-of-words form (bow_doc),\n",
    "    return the topic id with the highest probability.\n",
    "    \"\"\"\n",
    "    topic_probs = model.get_document_topics(bow_doc)\n",
    "    if not topic_probs:\n",
    "        return None\n",
    " \n",
    "    dominant_topic_id, max_prob = max(topic_probs, key=lambda x: x[1])\n",
    "    return dominant_topic_id\n",
    "\n",
    "\n",
    "dominant_topics = [get_dominant_topic(lda_model, bow_doc) for bow_doc in corpus]\n",
    "\n",
    "# Joined our original dataframe so labels + topics live in one place\n",
    "df[\"Dominant_Topic\"] = dominant_topics\n",
    "\n",
    "df[[\"Title\", \"Label\", \"Dominant_Topic\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f904e24-082a-41f5-ab0c-498118c76447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8 - TABLE: TRUE LABEL vs DOMINANT LDA TOPIC\n",
    "\n",
    "# Raw counts\n",
    "topic_label_counts = pd.crosstab(df[\"Label\"], df[\"Dominant_Topic\"])\n",
    "display(topic_label_counts)\n",
    "\n",
    "# Row-wise percentages (how each label is distributed over topics)\n",
    "topic_label_pct = pd.crosstab(df[\"Label\"],\n",
    "                              df[\"Dominant_Topic\"],\n",
    "                              normalize=\"index\") * 100\n",
    "topic_label_pct = topic_label_pct.round(1)\n",
    "display(topic_label_pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f467bdd-3503-4603-a22e-029a7f307f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_Breaches has some overlap with topics 3 and 5, so the classifier later might confuse these.\n",
    "# Malware spans across four topics, so ML models will struggle the most here.\n",
    "\n",
    "#Most Confusing / Overlapping Labels\n",
    "# - Cyber_Attack - Malware (topics 1 & 3)\n",
    "# - Data_Breaches - Cyber_Attack (topic 3)\n",
    "#- Vulnerability - Malware (topic 5)\n",
    "\n",
    "# Cleanest label\n",
    "# - Vulnerability (strong Topic 0 dominance)\n",
    "\n",
    "#Hardest label for modeling\n",
    "# - Malware (very spread out - high overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec524ff-ba5d-47a3-97e9-383e4263852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9 - VISUALIZATIONS\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# VISUAL 1: Heatmap (raw counts)\n",
    "ct = pd.crosstab(df[\"Label\"], df[\"Dominant_Topic\"])\n",
    "\n",
    "ct.columns = [f\"Topic {i+1}\" for i in ct.columns]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(ct, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.title(\"Heatmap: True Label vs Dominant LDA Topic\")\n",
    "plt.xlabel(\"LDA Topic\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# VISUAL 2: Percentage heatmap \n",
    "ct_pct = pd.crosstab(\n",
    "    df[\"Label\"],\n",
    "    df[\"Dominant_Topic\"],\n",
    "    normalize=\"index\"\n",
    ") * 100\n",
    "\n",
    "ct_pct.columns = [f\"Topic {i+1}\" for i in ct_pct.columns]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(ct_pct.round(1), annot=True, cmap=\"Greens\", fmt=\".1f\")\n",
    "plt.title(\"Percentage Distribution of LDA Topics per True Label\")\n",
    "plt.xlabel(\"LDA Topic\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# VISUAL 3: Bar plot of topic frequencies\n",
    "\n",
    "topic_counts = df[\"Dominant_Topic\"].value_counts().sort_index()\n",
    "topic_labels = [f\"Topic {i+1}\" for i in topic_counts.index]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=topic_labels, y=topic_counts.values)\n",
    "plt.title(\"Overall Distribution of LDA Topics\")\n",
    "plt.xlabel(\"LDA Topic\")\n",
    "plt.ylabel(\"Document Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a47bba5-e0c6-4949-bc00-a811199d3dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
